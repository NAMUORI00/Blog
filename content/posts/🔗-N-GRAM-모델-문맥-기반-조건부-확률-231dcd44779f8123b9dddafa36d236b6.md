---
title: "🔗 N-GRAM 모델 - 문맥 기반 조건부 확률"
date: "2025-07-15T04:14:00.000Z"
lastmod: "2025-07-23T06:19:00.000Z"
draft: false
series: []
tags: []
categories: []
authors:
  - "KYS"
NOTION_METADATA:
  object: "page"
  id: "231dcd44-779f-8123-b9dd-dafa36d236b6"
  created_time: "2025-07-15T04:14:00.000Z"
  last_edited_time: "2025-07-23T06:19:00.000Z"
  created_by:
    object: "user"
    id: "f96e6171-8ea1-4a4a-82e4-72ba4441b8c0"
  last_edited_by:
    object: "user"
    id: "f96e6171-8ea1-4a4a-82e4-72ba4441b8c0"
  cover: null
  icon: null
  parent:
    type: "database_id"
    database_id: "218dcd44-779f-81dd-bf93-e4fadbccbfde"
  archived: false
  in_trash: false
  properties:
    series:
      id: "B%3C%3FS"
      type: "multi_select"
      multi_select: []
    draft:
      id: "JiWU"
      type: "checkbox"
      checkbox: false
    authors:
      id: "bK%3B%5B"
      type: "people"
      people: []
    custom-front-matter:
      id: "c~kA"
      type: "rich_text"
      rich_text: []
    tags:
      id: "jw%7CC"
      type: "multi_select"
      multi_select: []
    categories:
      id: "nbY%3F"
      type: "multi_select"
      multi_select: []
    Last edited time:
      id: "vbGE"
      type: "last_edited_time"
      last_edited_time: "2025-07-23T06:19:00.000Z"
    summary:
      id: "x%3AlD"
      type: "rich_text"
      rich_text: []
    Name:
      id: "title"
      type: "title"
      title:
        - type: "text"
          text:
            content: "🔗 N-GRAM 모델 - 문맥 기반 조건부 확률"
            link: null
          annotations:
            bold: false
            italic: false
            strikethrough: false
            underline: false
            code: false
            color: "default"
          plain_text: "🔗 N-GRAM 모델 - 문맥 기반 조건부 확률"
          href: null
  url: "https://www.notion.so/N-GRAM-231dcd44779f8123b9dddafa36d236b6"
  public_url: "https://namuori00.notion.site/N-GRAM-231dcd44779f8123b9dddafa36d236b6"
MANAGED_BY_NOTION_HUGO: true

---


# 🔗 N-GRAM 모델 - 문맥 기반 조건부 확률


## N-GRAM의 철학적 기초


> **"현재 단어의 확률은 바로 앞의 N-1개 단어에만 의존한다"**


```javascript
기본 가정 - 마르코프 성질:
P(wᵢ|w₁, w₂, ..., wᵢ₋₁) ≈ P(wᵢ|wᵢ₋ₙ₊₁, ..., wᵢ₋₁)

핵심 아이디어:
• 제한된 문맥 윈도우로 언어의 복잡성 근사
• 통계적 규칙성을 이용한 예측 모델
• 문맥 정보를 명시적으로 활용
```


## N-GRAM 모델의 수학적 정의


### 1. Unigram (1-gram): 독립 가정


```javascript
P(w₁, w₂, ..., wₙ) = ∏ᵢ₌₁ⁿ P(wᵢ)

예시: "I love NLP"
P("I love NLP") = P("I") × P("love") × P("NLP")
                = 0.05 × 0.02 × 0.001 = 0.000001

특징:
• 문맥 완전 무시
• 계산 단순, 메모리 효율적
• 현실적 언어 모델링 부정확
```


### 2. Bigram (2-gram): 직전 단어 의존


```javascript
P(wᵢ|wᵢ₋₁) = Count(wᵢ₋₁, wᵢ) / Count(wᵢ₋₁)

예시 계산:
"I love" 뒤에 "NLP"가 올 확률
P(NLP|love) = Count("love NLP") / Count("love")
            = 150 / 50,000 = 0.003

전체 문장 확률:
P("I love NLP") = P(I|<s>) × P(love|I) × P(NLP|love) × P(</s>|NLP)
                = 0.02 × 0.05 × 0.003 × 0.8 = 0.0000024
```


### 3. Trigram (3-gram): 직전 두 단어 의존


```javascript
P(wᵢ|wᵢ₋₂, wᵢ₋₁) = Count(wᵢ₋₂, wᵢ₋₁, wᵢ) / Count(wᵢ₋₂, wᵢ₋₁)

예시:
P(NLP|I, love) = Count("I love NLP") / Count("I love")
               = 25 / 1,200 = 0.021

장점: 더 정확한 문맥 포착
단점: 데이터 희소성 증가
```


## 🔧 데이터 희소성 해결 - 스무딩 알고리즘


### 희소성 문제의 본질


```javascript
문제 상황:
훈련 코퍼스에서 "quantum computing"은 50번 등장
하지만 "quantum computer"는 0번 등장

Maximum Likelihood 추정:
P(computer|quantum) = Count("quantum computer") / Count("quantum") = 0/50 = 0

결과: 완전히 합리적인 문장이 확률 0으로 판정
→ 언어 모델의 치명적 결함
```


### 1. Add-k (Laplace) 스무딩


```python
def add_k_smoothing(bigram_counts, unigram_counts, k=1, vocab_size=50000):
    """Add-k 스무딩 적용"""
    smoothed_probs = {}
    
    for context in unigram_counts:
        for word in vocab:
            # 원본 카운트
            bigram_count = bigram_counts.get((context, word), 0)
            context_count = unigram_counts[context]
            
            # 스무딩 적용
            smoothed_prob = (bigram_count + k) / (context_count + k * vocab_size)
            smoothed_probs[(context, word)] = smoothed_prob
    
    return smoothed_probs

실제 예시:
원본: P(computer|quantum) = 0/50 = 0
Add-1: P(computer|quantum) = (0+1)/(50+1×50000) = 1/50050 ≈ 0.00002
```


### 2. Good-Turing 스무딩


```javascript
핵심 아이디어:
"r번 관찰된 이벤트의 실제 확률은 (r+1)번 관찰된 이벤트 수와 관련있다"

조정된 카운트:
c* = (c + 1) × N_{c+1} / N_c

여기서:
• c: 원본 카운트
• N_c: 정확히 c번 관찰된 이벤트 수
• c*: 조정된 카운트

실제 데이터 예시 (영어 1M 단어 코퍼스):
N₁ = 32,242개 (1번만 등장하는 단어)
N₂ = 9,833개 (2번 등장하는 단어)
N₃ = 4,562개 (3번 등장하는 단어)

미관측 단어 확률: N₁/총단어수 = 32,242/1,000,000 = 0.032
```


### 3. Kneser-Ney 스무딩


```javascript
혁신적 아이디어:
"단어의 확률은 그 단어가 등장하는 문맥의 다양성과 관련있다"

예시:
"Francisco"는 빈도 높지만 주로 "San Francisco"에서만 등장
"door"는 상대적으로 빈도 낮지만 다양한 문맥에서 등장
→ "door"가 새로운 문맥에서 등장할 확률이 더 높음

핵심: 절대 빈도보다 문맥 다양성이 중요
```


## N-GRAM의 토크나이제이션에서의 역할


### 1. 품질 평가 메트릭


```python
def ngram_overlap_score(reference_tokens, candidate_tokens, n=2):
    """N-gram 격침도로 토크나이제이션 품질 평가"""
    ref_ngrams = set(get_ngrams(reference_tokens, n))
    cand_ngrams = set(get_ngrams(candidate_tokens, n))
    
    overlap = len(ref_ngrams & cand_ngrams)
    total = len(ref_ngrams | cand_ngrams)
    
    return overlap / total if total > 0 else 0

사용 예:
인간 분할: ["machine", "learning", "is", "great"]
BPE 분할: ["mach", "ine", "learn", "ing", "is", "great"]
Bigram 격침도: 0.67 (양호)
```


### 2. 언어 식별 (Language Identification)


```python
def language_identification_ngram(text, lang_models, n=3):
    """Character N-gram을 이용한 언어 식별"""
    text_ngrams = get_character_ngrams(text, n)
    
    scores = {}
    for lang, model in lang_models.items():
        score = 0
        for ngram in text_ngrams:
            score += math.log(model.get(ngram, 1e-10))
        scores[lang] = score
    
    return max(scores, key=scores.get)

정확도:
• Trigram 기반: 99.2% (50개 언어)
• 매우 짧은 텍스트(10자)도 90% 정확도
• 혼재 언어 텍스트 감지 가능
```


## 현대 NLP에서 N-GRAM의 한계와 의의


### 근본적 한계


```javascript
희소성 문제 (Sparsity):
• 새로운 N-gram 조합은 확률 0
• 긴 문맥일수록 학습 데이터에서 누락
• 일반화 능력 부족

독립성 가정의 한계:
• 장거리 의존성 무시
• 문법적/의미적 제약 반영 부족
• 계층적 구조 모델링 불가

지수적 복잡도:
• 어휘 크기 V, N-gram 차수 n에 대해 O(V^n)
• 메모리 요구사항 급증
• 현실적 한계: 보통 3-4gram까지
```


### 현대적 의의


```javascript
신경망 언어 모델의 기초:
• Transformer 이전 주력 기법
• 현재도 백오프 메커니즘으로 활용
• 빠른 추론 속도로 실시간 응용에 사용

토크나이제이션 품질 관리:
• 자동 평가 메트릭
• 일관성 검증 도구
• 언어 식별 및 분류

하이브리드 시스템:
• 신경망 + N-gram 조합
• 소규모 데이터에서 안정성 제공
• 해석 가능한 백업 모델
```


## 서브워드 토크나이제이션에서의 N-GRAM 역할


### 패턴 검증


```python
def validate_subword_patterns(tokens, ngram_model):
    """N-gram 모델로 서브워드 분할 품질 검증"""
    total_prob = 0
    
    for i in range(len(tokens) - 1):
        bigram_prob = ngram_model.get_prob(tokens[i], tokens[i+1])
        total_prob += math.log(bigram_prob)
    
    return total_prob / len(tokens)  # 평균 로그 확률

용도:
• BPE vs UNIGRAM 분할 품질 비교
• 이상한 분할 패턴 자동 감지
• 토크나이제이션 모델 튜닝
```


### 언어별 적응


```javascript
언어        최적 스무딩       이유
영어        Modified KN      풍부한 데이터, 복잡한 패턴
중국어      Kneser-Ney       문자 조합 패턴 중요
한국어      Good-Turing      교착어 특성, 희소성 높음
아랍어      Add-k            형태 변화 복잡, 단순 백오프
```

