---
title: "🎯 UNIGRAM 언어 모델 - 확률론적 최적화 접근"
date: "2025-07-15T04:11:00.000Z"
lastmod: "2025-07-23T06:20:00.000Z"
draft: false
series: []
tags: []
categories: []
authors:
  - "KYS"
NOTION_METADATA:
  object: "page"
  id: "231dcd44-779f-81c8-b670-c73b084f9e4a"
  created_time: "2025-07-15T04:11:00.000Z"
  last_edited_time: "2025-07-23T06:20:00.000Z"
  created_by:
    object: "user"
    id: "f96e6171-8ea1-4a4a-82e4-72ba4441b8c0"
  last_edited_by:
    object: "user"
    id: "f96e6171-8ea1-4a4a-82e4-72ba4441b8c0"
  cover: null
  icon: null
  parent:
    type: "database_id"
    database_id: "218dcd44-779f-81dd-bf93-e4fadbccbfde"
  archived: false
  in_trash: false
  properties:
    series:
      id: "B%3C%3FS"
      type: "multi_select"
      multi_select: []
    draft:
      id: "JiWU"
      type: "checkbox"
      checkbox: false
    authors:
      id: "bK%3B%5B"
      type: "people"
      people: []
    custom-front-matter:
      id: "c~kA"
      type: "rich_text"
      rich_text: []
    tags:
      id: "jw%7CC"
      type: "multi_select"
      multi_select: []
    categories:
      id: "nbY%3F"
      type: "multi_select"
      multi_select: []
    Last edited time:
      id: "vbGE"
      type: "last_edited_time"
      last_edited_time: "2025-07-23T06:20:00.000Z"
    summary:
      id: "x%3AlD"
      type: "rich_text"
      rich_text: []
    Name:
      id: "title"
      type: "title"
      title:
        - type: "text"
          text:
            content: "🎯 UNIGRAM 언어 모델 - 확률론적 최적화 접근"
            link: null
          annotations:
            bold: false
            italic: false
            strikethrough: false
            underline: false
            code: false
            color: "default"
          plain_text: "🎯 UNIGRAM 언어 모델 - 확률론적 최적화 접근"
          href: null
  url: "https://www.notion.so/UNIGRAM-231dcd44779f81c8b670c73b084f9e4a"
  public_url: "https://namuori00.notion.site/UNIGRAM-231dcd44779f81c8b670c73b084f9e4a"
MANAGED_BY_NOTION_HUGO: true

---


# 🎯 UNIGRAM 언어 모델 - 확률론적 최적화 접근


## 기본 철학 및 패러다임 전환


> **"각 서브워드가 독립적으로 생성된다고 가정하고, 전체 시퀀스의 확률을 최대화하는 분할을 찾는다"**


```javascript
BPE의 한계 극복:
BPE: 탐욕적 병합 (지역 최적화)
     ↓
UNIGRAM: 전역 확률 최적화
         확률적 다중 분할 고려
         이론적 근거 있는 접근
```


## 수학적 기초 체계


### 1. 기본 확률 모델


```javascript
주어진 문장 S를 토큰 시퀀스 x = (x₁, x₂, ..., xₙ)로 분할할 때:

P(x) = ∏ᵢ₌₁ⁿ P(xᵢ)

여기서:
• P(xᵢ): 토큰 xᵢ의 독립 확률
• 각 토큰은 다른 토큰과 독립적으로 생성
• 전체 확률 = 개별 확률의 곱
```


### 2. 최적화 목표 함수


```javascript
목표: argmax P(x|S) = argmax ∏ᵢ₌₁ⁿ P(xᵢ)

로그 변환 (수치 안정성):
argmax ∑ᵢ₌₁ⁿ log P(xᵢ)

실제 구현:
L(x) = ∑ᵢ₌₁ⁿ log P(xᵢ) + λ × |x|

여기서:
• λ: 토큰 길이 페널티 (과도한 분할 방지)
• |x|: 토큰 시퀀스 길이
```


### 3. 독립성 가정의 의미와 함의


```javascript
독립성 가정:
P(xᵢ|x₁, ..., xᵢ₋₁, xᵢ₊₁, ..., xₙ) = P(xᵢ)

장점:
• 계산 복잡도 대폭 감소
• 병렬 처리 가능
• 수학적 해석 용이

단점:
• 문맥 정보 무시
• 장거리 의존성 손실
• 조건부 확률 무시

완화 방법:
• Subword regularization 도입
• Multiple segmentation 고려
• 확률적 샘플링 활용
```


## BPE vs UNIGRAM 패러다임 비교


| **측면**     | **BPE**             | **UNIGRAM**         |
| ---------- | ------------------- | ------------------- |
| **접근 방식**  | 빈도 기반 병합            | 확률론적 최적화            |
| **방향성**    | Bottom-up (문자→단어)   | Top-down (전체→부분)    |
| **기준**     | 지역적 빈도 최대화          | 전역적 확률 최대화          |
| **과정**     | 결정적 (deterministic) | 확률적 (probabilistic) |
| **목표**     | 탐욕적 지역 최적화          | 이론적 전역 최적화 추구       |
| **분할 결과**  | 단일 고정 분할            | 확률 분포로 표현           |
| **수학적 근거** | 휴리스틱                | 정보 이론 기반            |


## 🔄 EM 알고리즘을 통한 확률 추정


### EM 알고리즘 개요


> **"관찰되지 않은 잠재 변수가 있는 상황에서 최대 우도 추정을 수행하는 반복 알고리즘"**


```javascript
UNIGRAM에서의 적용:
관찰된 데이터: 원시 텍스트 문장들
잠재 변수: 각 문장의 올바른 토큰 분할
목표: 토큰별 확률 P(token) 추정
```


### E-step (Expectation): 사후 확률 계산


**1. 수학적 정의**


```javascript
현재 모델 θ⁽ᵗ⁾로 각 문장의 가능한 분할에 대한 사후 확률 계산:

P(segmentation|sentence, θ⁽ᵗ⁾) = 
    P(sentence|segmentation, θ⁽ᵗ⁾) × P(segmentation|θ⁽ᵗ⁾) / Z

여기서:
• P(sentence|segmentation, θ⁽ᵗ⁾) = ∏ᵢ P(tokenᵢ|θ⁽ᵗ⁾)
• Z = ∑ₛₑᗨ P(sentence|seg, θ⁽ᵗ⁾) (정규화 상수)
```


**2. 효율적 구현: Forward-Backward 알고리즘**


```python
def efficient_e_step(sentence, vocab, probs):
    """Dynamic Programming을 사용한 효율적 E-step"""
    n = len(sentence)
    
    # Forward pass: α[i] = P(sentence[0:i]의 최적 분할)
    alpha = [0.0] * (n + 1)
    alpha[0] = 1.0
    
    for i in range(1, n + 1):
        for j in range(i):
            substring = sentence[j:i]
            if substring in vocab:
                alpha[i] += alpha[j] * probs[substring]
    
    # Backward pass: β[i] = P(sentence[i:n]의 최적 분할)
    beta = [0.0] * (n + 1)
    beta[n] = 1.0
    
    for i in range(n - 1, -1, -1):
        for j in range(i + 1, n + 1):
            substring = sentence[i:j]
            if substring in vocab:
                beta[i] += probs[substring] * beta[j]
    
    # 각 토큰의 기댁값 계산
    expected_counts = defaultdict(float)
    for i in range(n):
        for j in range(i + 1, n + 1):
            substring = sentence[i:j]
            if substring in vocab:
                posterior = (alpha[i] * probs[substring] * beta[j]) / alpha[n]
                expected_counts[substring] += posterior
    
    return expected_counts
```


### M-step (Maximization): 확률 업데이트


```javascript
E-step에서 계산된 기댑값을 이용해 토큰 확률 업데이트:

P⁽ᵗ⁺¹⁾(tokenᵢ) = Expected_count(tokenᵢ) / Total_expected_count

여기서:
• Expected_count(tokenᵢ): E-step에서 계산된 토큰 i의 기댡값
• Total_expected_count: 모든 토큰의 기댁값 합
```


## ✂️ 가지치기 과정


### 가지치기의 필요성과 목표


```javascript
초기 문제:
• 시작 어휘: 모든 가능한 서브스트링 (100만개+)
• 목표 어휘: 32K 토큰 (메모리/속도 제약)
• 도전: 성능 손실 최소화하며 97% 어휘 축소

핵심 원리:
"정보 손실이 가장 적은 토큰부터 순차적으로 제거"
```


### 손실 함수 기반 가지치기


```python
def calculate_likelihood_loss(sentences, vocab_without_token, current_probs):
    """특정 토큰 제거 시 우도 손실 계산"""
    total_loss = 0
    
    for sentence in sentences:
        # 원래 최적 분할
        original_segmentation = find_best_segmentation(sentence, vocab_with_token, current_probs)
        original_logprob = sum(math.log(current_probs[token]) for token in original_segmentation)
        
        # 토큰 제거 후 최적 분할
        new_segmentation = find_best_segmentation(sentence, vocab_without_token, current_probs)
        new_logprob = sum(math.log(current_probs[token]) for token in new_segmentation)
        
        # 손실 누적
        total_loss += original_logprob - new_logprob
    
    return total_loss
```


### 가지치기 전략별 비교


```javascript
1. 무작위 제거:
   • 시간: 매우 빠름 (O(|V|))
   • 성능: 최악 (-15% BLEU)
   • 사용: 기준선 비교용

2. 빈도 기반 제거:
   • 시간: 빠름 (O(|V| log |V|))
   • 성능: 나쁨 (-8% BLEU)
   • 문제: 희귀하지만 중요한 토큰 제거

3. 손실 기반 탐욕적:
   • 시간: 느림 (O(|V|² × N))
   • 성능: 좋음 (-2% BLEU)
   • 사용: 중간 규모 데이터

4. 적응적 손실 기반:
   • 시간: 보통 (O(|V|^1.5 × N))
   • 성능: 최고 (-0.5% BLEU)
   • 사용: 대규모 프로덕션

5. 계층적 가지치기:
   • 빈도 → 손실 → 미세조정 순서
   • 속도와 성능의 균형
   • 가장 실용적인 접근법
```

